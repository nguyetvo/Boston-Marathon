{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Nguyet Vo_Boston_Marathon.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nguyetvo/Boston-Marathon/blob/master/Nguyet_Vo_Boston_Marathon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_7aZiFLcLHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import KMeans, MeanShift, SpectralClustering, AffinityPropagation\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-BL-EMhXcLHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "site = 'https://raw.githubusercontent.com/llimllib/bostonmarathon/master/results/2014/results.csv'\n",
        "data = pd.read_csv(site, sep=',')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "bSO7Gai2cLHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "s6y288DKcLHm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(data.shape)\n",
        "print(data.shape[0]/4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVui-g4CcLHs",
        "colab_type": "text"
      },
      "source": [
        "So our rows divide evenly into 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "xdz4LyNmcLHu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.columns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9XYd0nwcLHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in data.columns:\n",
        "    print(column)\n",
        "    print(data[column].isna().value_counts())\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adWLg7eBcLH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.ctz.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0mtkcEucLH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.state.unique()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMqFTfz-cLH5",
        "colab_type": "text"
      },
      "source": [
        "Not sure if these are appropriate measures, but I plan to drop the \"ctz\" column, because most of its values are null, and convert all null values of the \"state\" column to 'unknown.'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4KzYIuicLH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Drop the \"ctz\" column, replace nan in \"state\" column with 'unknown', replace '-' in the\n",
        "# \"_k\" columns with 0\n",
        "data.drop('ctz', axis=1, inplace=True)\n",
        "data.replace(np.nan, 'unknown', inplace=True, regex=True)\n",
        "data.replace('-', 0, inplace=True, regex=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyZZYLA2cLH8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setup X and y variables for modeling, drop name from predictors as it will not be helpful\n",
        "X = data.drop(['overall', 'name'], axis=1)\n",
        "\n",
        "# Normalize both target and predictors\n",
        "X_norm = MinMaxScaler().fit_transform(X[['10k', 'division', '25k', 'age', 'official', 'genderdiv', '35k', 'pace', '30k', '5k', 'half', '20k', '40k']])\n",
        "y_norm = MinMaxScaler().fit_transform(data[['overall']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-g5a0FQcLH_",
        "colab_type": "text"
      },
      "source": [
        "### Divide data into 4 even subsets to test for model consistency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGkKNQTUcLIA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Perform PCA for graphic representation\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_norm)\n",
        "\n",
        "# Create 4 equal data subsets to test our clustering and models on\n",
        "X_1, X_2, X_pca1, X_pca2 = train_test_split(X_norm, X_pca, test_size=0.5, random_state=5588)\n",
        "\n",
        "# Create final groupings\n",
        "X1, X2, XPCA1, XPCA2 = train_test_split(X_1, X_pca1, test_size=0.5, random_state=5588)\n",
        "X3, X4, XPCA3, XPCA4 = train_test_split(X_2, X_pca2, test_size=0.5, random_state=5588)\n",
        "\n",
        "subsets = [X1, X2, X3, X4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgxj9RK4cLIC",
        "colab_type": "text"
      },
      "source": [
        "### Run KMeans consistency analysis with plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZlMdDiYcLID",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ypred = pd.DataFrame()\n",
        "\n",
        "for counter, vals in enumerate([\n",
        "    (X1, XPCA1),\n",
        "    (X2, XPCA2),\n",
        "    (X3, XPCA3),\n",
        "    (X4, XPCA4)]):\n",
        "    \n",
        "    # Put the features into ypred.\n",
        "    ypred['pca_f1' + '_sample' + str(counter)] = vals[1][:, 0]\n",
        "    ypred['pca_f2' + '_sample' + str(counter)] = vals[1][:, 1]\n",
        "    \n",
        "    # Generate cluster predictions and store them for clusters 2 to 6.\n",
        "    for nclust in range(2, 7):\n",
        "        pred = KMeans(n_clusters=nclust, random_state=5588).fit_predict(vals[0])\n",
        "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "iBqWiU-ScLIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each  number of clusters, plot the clusters using the\n",
        "# pca features for each sample.\n",
        "for cluster in range(2, 7):\n",
        "    \n",
        "    # Make a grid of subplots.\n",
        "    f, axarr = plt.subplots(2, 2)\n",
        "    \n",
        "    # Make a plot for each sample.\n",
        "    for i in range(4):\n",
        "        \n",
        "        # PCA-created features.\n",
        "        x_sub = ypred['pca_f1_sample{}'.format(i)]\n",
        "        y_sub = ypred['pca_f2_sample{}'.format(i)]\n",
        "        \n",
        "        # Cluster assignments.\n",
        "        c = ypred['clust{}_sample{}'.format(cluster, i)]\n",
        "        \n",
        "        # Assign the subplot to its place on the grid.\n",
        "        rows = int(np.floor(i / 2))\n",
        "        cols = i % 2\n",
        "        axarr[rows, cols].scatter(x_sub, y_sub, c=c)\n",
        "        axarr[rows, cols].set_title('sample {}'.format(i))\n",
        "        axarr[rows, cols].set_xlim([-1, 1])\n",
        "        axarr[rows, cols].set_ylim([-1, 1])\n",
        "    \n",
        "    # Space out the plots so that the headings don't overlap axis values.\n",
        "    plt.suptitle('{} Clusters'.format(cluster), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwBtaY9cLII",
        "colab_type": "text"
      },
      "source": [
        "It appears to me that 3 clusters provides the best balance of number of clusters and consistency across data subsets. 4 clusters is also pretty good. Once you get above 4 clusters, boundaries start to shift a lot more.\n",
        "\n",
        "### I don't see any means of measuring ARI on this data set, as we are using a numerical variable, so I will move on to assessing the Silhouette Coefficient using KMeans"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CL_2CQjHcLII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for cluster in range(2, 7 ):\n",
        "    print()\n",
        "    print(\"{} clusters:\".format(cluster))\n",
        "    for sample in subsets:\n",
        "        model = KMeans(n_clusters=cluster, random_state=42).fit(sample)\n",
        "        labels = model.labels_\n",
        "        print(metrics.silhouette_score(sample, labels, metric='euclidean'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3UjqZXecLIK",
        "colab_type": "text"
      },
      "source": [
        "### Looks like the 3 cluster model wins just slightly over the 2 cluster. The scores are all pretty consistent across the 4 subsets of the data as well, which is positive.\n",
        "\n",
        "### KMeans Elbow Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "7BzhQ0hFcLIL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loop through all 4 subsets of the data\n",
        "it = 1\n",
        "K = range(1, 10)\n",
        "for X in subsets:\n",
        "    # Determine distortion for each value of k clusters\n",
        "    distortions = []\n",
        "    for k in K:\n",
        "        k_mean_model = KMeans(n_clusters=k).fit(X)\n",
        "        k_mean_model.fit(X)  # Why do we need to fit X again after we already assign a fit model?\n",
        "        distortions.append(np.sum(np.min(cdist(X, k_mean_model.cluster_centers_, 'euclidean'),\n",
        "                                         axis=1))/X.shape[0])\n",
        "    \n",
        "    # Create coordinates for line segment to plot start point to end point\n",
        "    x1, y1 = 1, distortions[0]\n",
        "    x2, y2 = 9, distortions[-1]\n",
        "    \n",
        "    # Plot elbow\n",
        "    plt.plot(K, distortions, 'bx-')\n",
        "    plt.plot((x1, x2), (y1, y2))\n",
        "    plt.title('Elbow Method plot for data subset {}'.format(it))\n",
        "    plt.xlabel('# Clusters')\n",
        "    plt.ylabel('Distortion')\n",
        "    plt.show()\n",
        "    \n",
        "    # Determing optimal cluster size, best balance between within-cluster homogeneity and \n",
        "    # cluster diversity by finding point furthest from straight line (x1, y1), (x2, y2)\n",
        "    distances = []\n",
        "    for i in range(len(distortions)):\n",
        "        x0 = i + 1\n",
        "        y0 = distortions[i]\n",
        "        numerator = abs((y2 - y1) * x0 - (x2 - x1) * y0 + x2 * y1 - y2* x1)\n",
        "        denominator = ((y2 - y1)**2 + (x2 - x1)**2)**0.5\n",
        "        distances.append(numerator/denominator)\n",
        "    # Return best n_clusters value\n",
        "    print('Optimal number of clusters:', distances.index(max(distances)) + 1)\n",
        "    it += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS7wnX5ycLIO",
        "colab_type": "text"
      },
      "source": [
        "We have undecided elbow method plots between the 4 subsets of data. Based on all the methods together as well as the plotting above, I would say 3 clusters would be best. The Silhouette Coefficient tipped the scales.\n",
        "\n",
        "## Create KMeans model with ideal n_clusters and evaluate entired data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0oD3TY4cLIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "\n",
        "y_norm = np.array(list(itertools.chain.from_iterable(y_norm)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk_AuImOcLIT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_kmean = KMeans(n_clusters=3, init='k-means++', max_iter=300, n_init=10,\n",
        "                    tol=0.0001, n_jobs=-1, random_state=5588)\n",
        "y_pred = best_kmean.fit_predict(X_norm)\n",
        "\n",
        "# Plot the solution.\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
        "plt.show()\n",
        "\n",
        "# Check the solution against the data.\n",
        "print('Comparing k-means clusters against the data:')\n",
        "print(pd.crosstab(y_pred, y_norm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "lrt-9vikcLIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['cluster'] = y_pred + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7paJKcnqcLIa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgqRRbh_cLIi",
        "colab_type": "text"
      },
      "source": [
        "### Run Times (5k, 10k, 20k, half, 25k, 30k, 35k, 40k, official)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nUDAWGQcLIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Need all of our time columns to be numeric not objects\n",
        "dist_col = ['5k', '10k', '20k', 'half', '25k', '30k', '35k', '40k', 'official']\n",
        "for column in dist_col:\n",
        "    data[column] = data[column].astype(float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbF6rMmOcLIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 15))\n",
        "\n",
        "for dist in dist_col:\n",
        "    plt.subplot(3, 3, dist_col.index(dist)+1)\n",
        "    sns.boxplot(x='cluster', y=dist, data=data)\n",
        "    plt.title('{} times by cluster number'.format(dist))\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PWU4c1JcLIn",
        "colab_type": "text"
      },
      "source": [
        "The configuration of the boxplots remains impressively consistent across all measures of distance. Runners in cluster 3 tend to be much faster than cluster 1 or 2. Cluster 2 tends to be a bit faster than cluster 1. \n",
        "\n",
        "Next we'll look at gender, starting with the distribution for each cluster.\n",
        "\n",
        "### Gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvh37McTcLIn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to find percentage of men and women per cluster.\n",
        "gender = pd.DataFrame(columns=['F', 'M'])\n",
        "gender.loc['cluster_1'] = data[data['cluster']==1].groupby('gender').count()['cluster']/data[data['cluster']==1].count()['cluster']\n",
        "gender.loc['cluster_2'] = data[data['cluster']==2].groupby('gender').count()['cluster']/data[data['cluster']==2].count()['cluster']\n",
        "gender.loc['cluster_3'] = data[data['cluster']==3].groupby('gender').count()['cluster']/data[data['cluster']==3].count()['cluster']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Mhh_N88jcLIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Stacked horizontal bar chart of percentages\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(gender.index, gender.F, color='c', label='Female')\n",
        "plt.barh(gender.index, gender.M, left=gender.F, color='r', label='Male')\n",
        "plt.title('Gender distribution in each cluster')\n",
        "plt.ylabel('Cluster')\n",
        "plt.legend()\n",
        "\n",
        "# Countplot of clusters\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(y='cluster', data=data, order=[3, 2, 1])\n",
        "plt.title(\"Countplot of each cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxkQUL_hcLJS",
        "colab_type": "text"
      },
      "source": [
        "Interesting results here. We can see there are many more marathon participants who are male - clusters 2 and 3, both predominantly male, have the higher member counts. Female participants are only the majority in cluster 1, which has the lowest member count of the 3.\n",
        "\n",
        "I'm also curious to see what the run times plots we created above look like when the two genders are separated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8vtsG-FcLJT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 15))\n",
        "\n",
        "for dist in dist_col:\n",
        "    plt.subplot(3, 3, dist_col.index(dist)+1)\n",
        "    sns.boxplot(x='cluster', y=dist, hue='gender', data=data)\n",
        "    plt.title('{} times by cluster number'.format(dist))\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hR0ze_JqcLJX",
        "colab_type": "text"
      },
      "source": [
        "Here we see that male runners have consistently lower median times in all distance groupings than female runners for each of the 3 clusters. There is a greater gender discrepancy in cluster 3 than 1 or 2. And the IQR is shorter for cluster 3.\n",
        "\n",
        "We still haven't found anything that greatly differentiates clusters 1 and 2. Hopefully we'll find something next, as we continue our bivariate plots with the 'cluster' feature and the remaining columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEhRBvLycLJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 15))\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "sns.boxplot(x='cluster', y='age', data=data)\n",
        "plt.title('Median age for each cluster')\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "sns.boxplot(x='cluster', y='pace', data=data)\n",
        "plt.title('Cluster pace')\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "sns.boxplot(x='cluster', y='division', data=data)\n",
        "plt.title('Cluster division')\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "sns.boxplot(x='cluster', y='genderdiv', data=data)\n",
        "plt.title('Cluster gender division')\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "sns.boxplot(x='cluster', y='overall', data=data)\n",
        "plt.title('Overall ranking by cluster and gender')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47_4-M07cLJa",
        "colab_type": "text"
      },
      "source": [
        "### Age\n",
        "Cluster 1 represents participants under 40, cluster 2 is over 40, including outliers that stretch above 76/77 years old. Cluster 3 seems to run the entire range of ages but doesn't reach quite as high as cluster 2, having a few outliers as well going into the mid-70's.\n",
        "\n",
        "### Pace\n",
        "Pace reflects the same trend as the run times for each distance, as we would expect, being a measure of time taken to run a given distance. Cluster 3 is the lowest time, clusters 1 and 2 are very close, but 2 has a slightly lower median value. The boxplots have many outliers, all three have some above, cluster 3 is the only with outliers below the IQR.\n",
        "\n",
        "### Division\n",
        "The measure of a runner's rank in their division. Cluster 3 holds the lowest values or highest median division ranks, but cluster 2 is also very close. Here is another clear differentiation of clusters 1 and 2: 2 tends toward better division rank. Cluster 3 has a few outliers above the IQR.\n",
        "\n",
        "### Gender Division\n",
        "The measure of a runner's rank in their gender division. The gender division rank is pretty stable between clusters 1 and 2, but cluster 3 averages much lower values/higher ranks. Cluster 3 has a few outliers above the IQR.\n",
        "\n",
        "### Overall\n",
        "Overall rank of racers for the clusters follows the same trend as times: cluster 3 has the lowest values (highest ranks), clusters 1 and 2 are a bit higher up on the plot, the latter having just a slightly lower median\n",
        "\n",
        "### State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "O0ij92TPcLJa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states = pd.DataFrame()\n",
        "states['state'] = sorted(data.state.unique())\n",
        "states['cluster_1'] = np.array(data[data['cluster']==0].groupby('state').count()['cluster'] / data.groupby('state').count()['cluster'])\n",
        "states['cluster_2'] = np.array(data[data['cluster']==1].groupby('state').count()['cluster'] / data.groupby('state').count()['cluster'])\n",
        "states['cluster_3'] = np.array(data[data['cluster']==2].groupby('state').count()['cluster'] / data.groupby('state').count()['cluster'])\n",
        "states.fillna(0, inplace=True)\n",
        "states.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BswLd35WcLJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "# Create cluster_1 bars\n",
        "plt.barh(states.state, states.cluster_1, color='#b5ffb9', edgecolor='white', label='Cluster 1')\n",
        "# Create cluster_2 bars\n",
        "plt.barh(states.state, states.cluster_2, left=list(states.cluster_1), color='#f9bc86', \n",
        "         edgecolor='white', label='Cluster 2')\n",
        "# Create cluster_3 bars\n",
        "plt.barh(states.state, states.cluster_3, left=[i+j for i,j in zip(states.cluster_1, \n",
        "        states.cluster_2)], color='#a3acff', edgecolor='white', label='Cluster 3')\n",
        " \n",
        "plt.yticks(range(len(states.index)), states.state)\n",
        "plt.ylabel(\"State\")\n",
        "plt.title('Percent of participants in each cluster by state')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bRGZoVUcLJg",
        "colab_type": "text"
      },
      "source": [
        "From the above plot, there's one interesting observation that stands out: the vast majority of participants in each state fall into cluster 3, the fastest group. However, Massachusetts and its most proximate two states, New Hampshire and Rhode Island, saw much higher rates of participation in clusters 1 and 2. \n",
        "\n",
        "I believe the story behind this is: people who are travelling from out of state to participate are going to be very serious about running. The bar to entry for local Bostonians and Massachusettsans is much lower, so many casual runners and such may join as a fun local activity. It's an interesting distinction the model makes, especially as I did not feed the 'state' column to my clustering model.\n",
        "\n",
        "There are a few outlier \"states,\" in which all points are only in one cluster. I have a feeling that these are caused by small sample sizes from these states, so I want to check into that:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGGmCUFFcLJj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['state'].value_counts(ascending=True).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyfhYnmYcLJl",
        "colab_type": "text"
      },
      "source": [
        "That pretty much sums up all the standout bars on the plot that didn't follow the typical distribution. The four bars with only one cluster represented were made up of fewer than 3 participants each! \n",
        "\n",
        "\n",
        "### Country"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odIvSia1cLJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countries = pd.DataFrame()\n",
        "countries['country'] = sorted(data.country.unique())\n",
        "countries['cluster_1'] = np.array(data[data['cluster']==1].groupby('country').count()['cluster'] / data.groupby('country').count()['cluster'])\n",
        "countries['cluster_2'] = np.array(data[data['cluster']==2].groupby('country').count()['cluster'] / data.groupby('country').count()['cluster'])\n",
        "countries['cluster_3'] = np.array(data[data['cluster']==3].groupby('country').count()['cluster'] / data.groupby('country').count()['cluster'])\n",
        "countries.fillna(0, inplace=True)\n",
        "countries.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS9vxCYncLJo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "# Create cluster_1 bars\n",
        "plt.barh(countries.country, countries.cluster_1, color='#b5ffb9', edgecolor='white', \n",
        "         label='Cluster 1')\n",
        "# Create cluster_2 bars\n",
        "plt.barh(countries.country, countries.cluster_2, left=list(countries.cluster_1), \n",
        "         color='#f9bc86', edgecolor='white', label='Cluster 2')\n",
        "# Create cluster_3 bars\n",
        "plt.barh(countries.country, countries.cluster_3, left=[i+j for i,j in zip(countries.cluster_1, \n",
        "        countries.cluster_2)], color='#a3acff', edgecolor='white', label='Cluster 3')\n",
        " \n",
        "plt.yticks(range(len(countries.index)), countries.country)\n",
        "plt.ylabel(\"Country\")\n",
        "plt.title('Percent of participants in each cluster by country')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ru0lFP5cLJq",
        "colab_type": "text"
      },
      "source": [
        "There is a much wider variety of distributions in the countries plot than the US states plot we looked at before this. Cluster 3 obviously has the highest percentage of global participants, but there are many countries with only participants in clusters 1 or 2. On average, cluster 1 has the least participants, probably because it's expensive to travel and at a younger age, people are likely to have less money. \n",
        "\n",
        "For the countries with only values in one cluster, I'm curious to see if that can be explained by runner counts:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfUY9k2ycLJr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['country'].value_counts(ascending=True).head(25)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7USSevVqcLJt",
        "colab_type": "text"
      },
      "source": [
        "Above, the majority of the one-cluster countries in the plot above are represented in the above 25 country list. That helps to make some sense of the variety in the cluster distributions for countries plot and assuage some of the worry of extreme outliers.\n",
        "\n",
        "## 3-cluster analysis\n",
        "\n",
        "The clusters our model created are indeed interesting. I'll do my best to sum up what characterizes each. Overall, it can be said that the run times for male runners were consistently shorter than for female runners. Otherwise:\n",
        "\n",
        "__Cluster 1__: Characterized by higher marathon run times for all measured distances. This is a younger group, from 18-40 years of age, that is 60% female. As expected, if their times are higher, their pace and division spots also tend to be higher than their counterparts. There are fewer participants in cluster 1 for almost every country and state. One notable exception is Massachusetts, the event location, and I explained earlier my reasoning for that. I think of cluster 1 as young, casual runners, mostly from the Boston/Massachusetts area.\n",
        "\n",
        "__Cluster 2__: Characterized by slightly lower marathon run times than cluster 1 for all measured distances, and around 60% male. It's almost as if clusters 1 and 2 are two halves of the same coin, cluster 1 is participants below 40, cluster 2 is participants above 40. We see one notable difference between the two clusters in the 'division' ranking, where cluster 2 is closer to cluster 3, averaging much higher ranks than cluster 1. As far as the 'state' and 'country' columns, we see a greater percentage of participants from cluster 2 than cluster 1, but not as many as cluster 3. As I mentioned above, this may have to do with even the most serious older runners averaging a slower pace than their serious young counterparts. Or it could be free time (retirement) and greater resources than young people who, most likely, are raising children or paying student debt, buying houses, etc. Cluster 2 could be summed up as older runners, mostly male, either competitive or casual, that is undecided.\n",
        "\n",
        "__Cluster 3__: I would call cluster 3 the competitive runners from around the world. The times for each distance are shorter by a considerable amount than the other two clusters, and the pace, division ranking, gender division ranking, and overall ranking reflect that. Additionally, we see that most participants from states other than Massachusetts and other countries fall into cluster 3 - the majority of people who travel for this marathon are serious runners."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mL3N1KGicLJt",
        "colab_type": "text"
      },
      "source": [
        "### Building a 4-cluster model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "diWwagCAcLJu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_kmean4 = KMeans(n_clusters=4, init='k-means++', max_iter=300, n_init=10,\n",
        "                    tol=0.0001, random_state=5588)#, n_jobs=-1)\n",
        "y_pred = best_kmean4.fit_predict(X_norm)\n",
        "\n",
        "# Plot the solution.\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y_pred)\n",
        "plt.show()\n",
        "\n",
        "# Check the solution against the data.\n",
        "print('Comparing k-means clusters against the data:')\n",
        "print(pd.crosstab(y_pred, y_norm))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3glV6AKcLJx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data['4cluster'] = y_pred + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxEYvh9xcLJz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 15))\n",
        "\n",
        "for dist in dist_col:\n",
        "    plt.subplot(3, 3, dist_col.index(dist)+1)\n",
        "    sns.boxplot(x='4cluster', y=dist, data=data)\n",
        "    plt.title('{} times by cluster number'.format(dist))\n",
        "    \n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQGXVLC8cLJ2",
        "colab_type": "text"
      },
      "source": [
        "Here, cluster 2 contains the fastest runners for all distances. It appears that clusters 1 and 4 are most similar to 1 and 2 from our 3 cluster analysis. Cluster 3 here seems to be a new addition: faster than 1 or 4, but not the fastest.\n",
        "\n",
        "### Gender"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8HuKGwTcLJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# need to find percentage of men and women per cluster.\n",
        "gender = pd.DataFrame(columns=['F', 'M'])\n",
        "gender.loc['cluster_1'] = data[data['4cluster']==1].groupby('gender').count()['4cluster']/data[data['4cluster']==1].count()['4cluster']\n",
        "gender.loc['cluster_2'] = data[data['4cluster']==2].groupby('gender').count()['4cluster']/data[data['4cluster']==2].count()['4cluster']\n",
        "gender.loc['cluster_3'] = data[data['4cluster']==3].groupby('gender').count()['4cluster']/data[data['4cluster']==3].count()['4cluster']\n",
        "gender.loc['cluster_4'] = data[data['4cluster']==4].groupby('gender').count()['4cluster']/data[data['4cluster']==4].count()['4cluster']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lK0ekEOOcLJ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "# Stacked horizontal bar chart of percentages\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.barh(gender.index, gender.F, color='c', label='Female')\n",
        "plt.barh(gender.index, gender.M, left=gender.F, color='r', label='Male')\n",
        "plt.title('Gender distribution in each cluster')\n",
        "plt.ylabel('Cluster')\n",
        "plt.legend()\n",
        "\n",
        "# Countplot of clusters\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(y='4cluster', data=data, order=[4, 3, 2, 1])\n",
        "plt.title(\"Countplot of each cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AidOMYomcLJ6",
        "colab_type": "text"
      },
      "source": [
        "Here we have a pretty similar breakdown of the clusters by gender. Again we have one smaller cluster (4) that is majority female, while the rest are majority male. We have one small (1) and two large (2, 3) majority male clusters.\n",
        "\n",
        "### Other columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VBa3zkGDcLJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12, 15))\n",
        "\n",
        "plt.subplot(3, 2, 1)\n",
        "sns.boxplot(x='4cluster', y='age', data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Median age for each cluster')\n",
        "\n",
        "plt.subplot(3, 2, 2)\n",
        "sns.boxplot(x='4cluster', y='pace', data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Cluster pace')\n",
        "\n",
        "plt.subplot(3, 2, 3)\n",
        "sns.boxplot(x='4cluster', y='division', data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Cluster division')\n",
        "\n",
        "plt.subplot(3, 2, 4)\n",
        "sns.boxplot(x='4cluster', y='genderdiv', data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Cluster gender division')\n",
        "\n",
        "plt.subplot(3, 2, 5)\n",
        "sns.boxplot(x='4cluster', y='overall', data=data)\n",
        "plt.xlabel('Cluster')\n",
        "plt.title('Overall ranking by cluster')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn1-IsERcLJ8",
        "colab_type": "text"
      },
      "source": [
        "### Age\n",
        "For age, we can see two clusters tend toward younger participants, two tend toward older participants. Clusters 1 and 4 seem to share a similar relatiomship to clusters 1 and 2 from our 3 cluster analysis: in cluster 4 all participants are below 40 years of age, and in cluster 1 they are all above 40 years of age. Cluster 2 tends to be younger than cluster 3 by about 15 years. So we have an older fast group (cluster 3), which is not as fast as our younger fast group (cluster 2).\n",
        "\n",
        "### Cluster Pace\n",
        "Pace again reflects the distribution from the distance subplots before this. Clusters 1 and 4 have the slowest paces, followed by cluster 3, then cluster 2 at the fastes paces.\n",
        "\n",
        "### Cluster Division\n",
        "This distribution looks a bit similar to our 3 cluster model. The division rankings don't seem to fall in line with the runner's time/speed so much. The median for cluster 2 is slightly lower than cluster 3, though, which represents our overall trend. Cluster 4 has the highest median division ranking. \n",
        "\n",
        "### Cluster Gender Division\n",
        "Gender Division follows the same distribution as the distances and pace: Cluster 2 in the lead, cluster 3 behind that, clusters 1 and 4 are the highest(slowest).\n",
        "\n",
        "### Overall Rank\n",
        "Overall ranking also follows the above distribution (cluster gender division), no surprises.\n",
        "\n",
        "### State"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt78RW2fcLJ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "states4 = pd.DataFrame()\n",
        "states4['state'] = sorted(data.state.unique())\n",
        "states4['cluster_1'] = np.array(data[data['4cluster']==1].groupby('state').count()['4cluster'] / data.groupby('state').count()['4cluster'])\n",
        "states4['cluster_2'] = np.array(data[data['4cluster']==2].groupby('state').count()['4cluster'] / data.groupby('state').count()['4cluster'])\n",
        "states4['cluster_3'] = np.array(data[data['4cluster']==3].groupby('state').count()['4cluster'] / data.groupby('state').count()['4cluster'])\n",
        "states4['cluster_4'] = np.array(data[data['4cluster']==4].groupby('state').count()['4cluster'] / data.groupby('state').count()['4cluster'])\n",
        "states4.fillna(0, inplace=True)\n",
        "states4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asp1av9RcLJ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "# Create cluster_1 bars\n",
        "plt.barh(states4.state, states4.cluster_1, color='#b5ffb9', edgecolor='white', label='Cluster 1')\n",
        "# Create cluster_2 bars\n",
        "plt.barh(states4.state, states4.cluster_2, left=list(states4.cluster_1), color='#f9bc86', \n",
        "        edgecolor='white', label='Cluster 2')\n",
        "# Create cluster_3 bars\n",
        "plt.barh(states4.state, states4.cluster_3, left=[i+j for i,j in zip(states4.cluster_1, \n",
        "        states4.cluster_2)], color='#a3acff', edgecolor='white', label='Cluster 3')\n",
        "# Create cluster_4 bars\n",
        "plt.barh(states4.state, states4.cluster_4, left=[i+j+k for i,j,k in zip(states4.cluster_1, \n",
        "        states4.cluster_2, states4.cluster_3)], color='#7958b3', edgecolor='white', \n",
        "        label='Cluster 3')\n",
        "    \n",
        "plt.yticks(range(len(states4.index)), states4.state)\n",
        "plt.ylabel(\"State\")\n",
        "plt.title('Percent of participants in each cluster by state')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUvkodU2cLKB",
        "colab_type": "text"
      },
      "source": [
        "We have fairly consistent distribution of the four clusters across different states/territories. The outlier states with only one or two clusters are the same as last time - only 5 states/territories that qualify. We see that most of the participants from each state fall into clusters 2 and 3, our two fastest clusters. Again, Massachusetts has higher ratios of slower/less competitive runners (clusters 1 and 4), as do neighboring states New Hampshire (NH) and Maine (MN).\n",
        "\n",
        "### Country"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mc8Cfm8cLKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "countries4 = pd.DataFrame()\n",
        "countries4['country'] = sorted(data.country.unique())\n",
        "countries4['cluster_1'] = np.array(data[data['4cluster']==1].groupby('country').count()['4cluster'] / data.groupby('country').count()['4cluster'])\n",
        "countries4['cluster_2'] = np.array(data[data['4cluster']==2].groupby('country').count()['4cluster'] / data.groupby('country').count()['4cluster'])\n",
        "countries4['cluster_3'] = np.array(data[data['4cluster']==3].groupby('country').count()['4cluster'] / data.groupby('country').count()['4cluster'])\n",
        "countries4['cluster_4'] = np.array(data[data['4cluster']==4].groupby('country').count()['4cluster'] / data.groupby('country').count()['4cluster'])\n",
        "countries4.fillna(0, inplace=True)\n",
        "countries4.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Qmds7cvcLKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 20))\n",
        "\n",
        "# Create cluster_1 bars\n",
        "plt.barh(countries4.country, countries4.cluster_1, color='#b5ffb9', edgecolor='white', \n",
        "         label='Cluster 1')\n",
        "# Create cluster_2 bars\n",
        "plt.barh(countries4.country, countries4.cluster_2, left=list(countries4.cluster_1), \n",
        "         color='#f9bc86', edgecolor='white', label='Cluster 2')\n",
        "# Create cluster_3 bars\n",
        "plt.barh(countries4.country, countries4.cluster_3, left=[i+j for i,j in zip(countries4.cluster_1, \n",
        "         countries4.cluster_2)], color='#a3acff', edgecolor='white', label='Cluster 3')\n",
        "# Create cluster_4 bars\n",
        "plt.barh(countries4.country, countries4.cluster_4, \n",
        "         left=[i+j+k for i,j,k in zip(countries4.cluster_1, countries4.cluster_2, \n",
        "         countries4.cluster_3)], color='#7958b3', edgecolor='white', label='Cluster 3')\n",
        "\n",
        "plt.yticks(range(len(countries4.index)), countries4.country)\n",
        "plt.ylabel(\"Country\")\n",
        "plt.title('Percent of participants in each cluster by country')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0O--WwScLKH",
        "colab_type": "text"
      },
      "source": [
        "With the countries we also see a similar distribution of clusters as we did with the 3 cluster model and the states. Clusters 2 and 3, which are characterized by lower times/faster runners, make up the majority of the participants from around the world. Again, this is sensible. Clusters 1 and 4 represent a much smaller proportion of the runners as they are both made up of slower runners. \n",
        "\n",
        "There are just over 20 bars represented by only one or two clusters, most of which are on the list we saw earlier when we did the 3 cluster analysis: most are just very small pools of participants.\n",
        "\n",
        "## Final 4 Cluster Analysis\n",
        "\n",
        "We saw some similar distributions among clusters in our 3 and 4 cluster models, but it's not so simple as splitting one of the old clusters into two new ones. There was some shuffling about of runners into different clusters, so we'll have to reassess what makes each cluster unique:\n",
        "\n",
        "__Cluster 1__: The smallest cluster in terms of number of rows, cluster 1 is over 60% male. These runners are the oldest group, with everyone being over 40 years old. The slowest runners (longest times) are members of this cluster and cluster 4. While the median run times are slightly higher for this cluster than cluster 4, the division rankings are much lower in cluster 1. But, the gender division rankings are the highest (worst). \n",
        "\n",
        "About 15% of US/Canadian participants are in cluster 1. Many countries send a high percentage of runners who are in cluster 1 (around 30%). Looking closer, some are represented by only a few runners, and I am inferring the many of the other countries tend to have higher GDP or middle/upper class populations. This includes many European countries (FRA, ITA, GER, DEN, POL, etc.), developed East Asian countries (JPN, KOR), and quickly developing Asian countries (CHN, IND).\n",
        "\n",
        "We can say this is a smaller, older, slower cluster that is mostly male. It represents many runners from Massachusetts. These may be more casual runners as they aren't as fast.\n",
        "\n",
        "__Cluster 2__: This cluster is the largest in number of members and the fastest (lowest times). The age range is quite wide, but does not contain the oldest participants (IQR stops at 60). Gender is the most balanced of any of the clusters, about 44% female/56% male. The largest number of participants from most states and most countries come from cluster 2. This cluster is more serious runners (which, understandably) makes up the largest group in an internationally renown marathon.\n",
        "\n",
        "__Cluster 3__: Cluster 3 is the second largest and contains the second-fastest group of runners. The age range is pretty spread out, but all above 30 and extends all the way up to some of the oldest. Pace, gender division, and overall ranking all represent the same trend as the run times: above cluster 2, below clusters 1 and 4. The regular division ranking is slightly higher than cluster 2, slightly lower than cluster 1, and much lower than cluster 4. Cluster 3 is the second most represented cluster for almost all states. Many countries also show a large proportion of cluster 3 runners, but there is much more variance between countries. Overall, this cluster is a large group of serious runners who are slightly older than cluster 2. I would say those two clusters represent more passionate/fast runners, and the greatest distinction is age (cluster 2 being younger, on average).\n",
        "\n",
        "__Cluster 4__: The final cluster is the only one that is majority female. It's a smaller cluster, and tends toward longer times, but not as long as cluster 1. This is the youngest cluster, with all participants below 40. This cluster has the highest gender division values (which may be related to its unique gender distribution). Otherwise its pace, division score, and overall ranking follow the same pattern as its times. For most states, cluster 4 is the least represented, Massachusetts being the largest exception. There are very few international runners in cluster 4, with some exceptions. Cluster 4 represents young, mostly female, casual/amature runners, many of whom are from Massachusetts/Boston area. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOxsJubAXG3W",
        "colab_type": "text"
      },
      "source": [
        "**Add more functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIqu_v84XSiP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize data frames\n",
        "ypred = pd.DataFrame()\n",
        "score = pd.DataFrame(columns=['cluster_pred','sil_score'])\n",
        "\n",
        "# Keep track of counts of the models and use data from the different folds\n",
        "for counter, data in enumerate([\n",
        "    (X1, XPCA1),\n",
        "    (X2, XPCA2),\n",
        "    (X3, XPCA3),\n",
        "    (X4, XPCA4)]):\n",
        "    \n",
        "    # Put the features into ypred.\n",
        "    ypred['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
        "    ypred['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
        "    \n",
        "    # Creating a list of possible number of clusters to test in kmeans.\n",
        "    for nclust in range(2, 6):\n",
        "       \n",
        "        # Instantiating and fit_predicting model to then add to data frame\n",
        "        kmeans = KMeans(n_clusters=nclust, random_state=42)\n",
        "        pred = kmeans.fit_predict(data[0])\n",
        "        ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred\n",
        "        \n",
        "        # Calculating silhouette scores for the data and adding that to the shilouette score\n",
        "        labels = kmeans.labels_\n",
        "        sscore = metrics.silhouette_score(data[0], labels, metric='euclidean')\n",
        "        score = score.append({'cluster_pred':'clust' + str(nclust) + '_sample' + str(counter), \n",
        "                              'silhouette_score':sscore}, ignore_index=True)\n",
        "\n",
        "# Sorting sihoilette scores\n",
        "score.sort_values(by='silhouette_score', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL2XrjdLXdMU",
        "colab_type": "text"
      },
      "source": [
        "A five-cluster system has the highest silhouette scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz8DQ4zOXfHI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each  number of clusters, plot the clusters using the\n",
        "# pca features for each sample.\n",
        "for cluster in range(2, 6):\n",
        "    \n",
        "    # Make a grid of subplots.\n",
        "    f, axarr = plt.subplots(2, 2)\n",
        "    \n",
        "    # Make a plot for each sample.\n",
        "    for i in range(4):\n",
        "        \n",
        "        # PCA-created features.\n",
        "        x_sub = ypred['pca_f1_sample{}'.format(i)]\n",
        "        y_sub = ypred['pca_f2_sample{}'.format(i)]\n",
        "        \n",
        "        # Cluster assignments.\n",
        "        c = ypred['clust{}_sample{}'.format(cluster, i)]\n",
        "        \n",
        "        # Assign the subplot to its place on the grid.\n",
        "        rows = int(np.floor(i / 2))\n",
        "        cols = i % 2\n",
        "        axarr[rows, cols].scatter(x_sub, y_sub, c=c)\n",
        "        axarr[rows, cols].set_title('sample {}'.format(i))\n",
        "        axarr[rows, cols].set_xlim([-.5, .5])\n",
        "        axarr[rows, cols].set_ylim([-.5, 1.2])\n",
        "    \n",
        "    # Space out the plots so that the headings don't overlap axis values.\n",
        "    plt.suptitle('{} Clusters'.format(cluster), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBXMmQoLXoNG",
        "colab_type": "text"
      },
      "source": [
        "**Mean-shift**\n",
        "\n",
        "For a mean-shift model, we will use a range of quantiles to create bandwidths from 0.1 to 0.4, calculating the Silhouette scores for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jz02t2_mXq7p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model imports\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "\n",
        "# Initialize new data frames\n",
        "ypred_ms = pd.DataFrame()\n",
        "score_ms = pd.DataFrame(columns=['cluster_pred','mean_shift', 'quantile'])\n",
        "\n",
        "# Keep track of counts of the models and use data from the different folds\n",
        "for counter, data in enumerate([X1, X2, X3, X4]):\n",
        "    # Creating a list of possible quantiles to test in mean shift.\n",
        "    for n in [0.1, 0.2, 0.3, 0.4]:\n",
        "        # Estimating number of clusters for data\n",
        "        bandwidth = estimate_bandwidth(data, quantile=n, n_samples=500)\n",
        "        # Ensuring all sets are the same lenght\n",
        "        data = data[:4013][:]\n",
        "        # Instantiating and fit_predicting model to then add to data frame\n",
        "        ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "        pred = ms.fit_predict(data)\n",
        "        labels = ms.labels_\n",
        "        cntrs = len(np.unique(labels))\n",
        "        ypred_ms['clust' + str(cntrs) + '_sample' + str(counter)] = pred\n",
        "        # Calculating silhouette scores for the data and adding that to the shilouette score\n",
        "        sscore = metrics.silhouette_score(data, labels, metric='euclidean')\n",
        "        score_ms = score_ms.append({'cluster_pred':'clust' + str(cntrs) + '_sample' + str(counter), \n",
        "                              'silhouette_score':sscore, 'quantile':n}, ignore_index=True)\n",
        "\n",
        "score_ms.sort_values(by='silhouette_score', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0v0s5TwXxXT",
        "colab_type": "text"
      },
      "source": [
        "The quantile of 0.4 calculated a high Silhouette score for sample 2 and generated 4 clusters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aXLiXUsiX47_",
        "colab_type": "text"
      },
      "source": [
        "**Spectral Clustering**\n",
        "\n",
        "For the spectral clustering model, we use a range of clusters from 2 to 5, and we calculate the corresponding Silhouette scores for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UYFM2N8X_74",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize data frames\n",
        "ypred_sc = pd.DataFrame()\n",
        "score_sc = pd.DataFrame(columns=['cluster_pred','silhouette_score'])\n",
        "\n",
        "# Keep track of counts of the models and use data from the different folds\n",
        "for counter, data in enumerate([\n",
        "    (X1, XPCA1),\n",
        "    (X2, XPCA2),\n",
        "    (X3, XPCA3),\n",
        "    (X4, XPCA4)]):\n",
        "    \n",
        "    # Put the features into ypred.\n",
        "    ypred_sc['pca_f1' + '_sample' + str(counter)] = data[1][:, 0]\n",
        "    ypred_sc['pca_f2' + '_sample' + str(counter)] = data[1][:, 1]\n",
        "    \n",
        "    # Creating a list of possible number of clusters to test in kmeans.\n",
        "    for nclust in range(2, 6):\n",
        "        # Instantiating and fit_predicting model to then add to data frame\n",
        "        sc = SpectralClustering(n_clusters=nclust)\n",
        "        pred = sc.fit_predict(data[0])\n",
        "        ypred_sc['clust' + str(nclust) + '_sample' + str(counter)] = pred\n",
        "        # Calculating silhouette scores for the data and adding that to the shilouette score\n",
        "        labels = sc.labels_\n",
        "        sscore_sc = metrics.silhouette_score(data[0], labels, metric='euclidean')\n",
        "        score_sc = score_sc.append({'cluster_pred':'clust' + str(nclust) + '_sample' + str(counter), \n",
        "                              'silhouette_score':sscore_sc}, ignore_index=True)\n",
        "\n",
        "score_sc.sort_values(by='silhouette_score', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nigsyPd2YKuq",
        "colab_type": "text"
      },
      "source": [
        "A 2 cluster configuration generates the highest silhouette score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdIxAoOSYUz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For each  number of clusters, plot the clusters using the\n",
        "# pca features for each sample.\n",
        "for cluster in range(2, 6):\n",
        "    \n",
        "    # Make a grid of subplots.\n",
        "    f, axarr = plt.subplots(2, 2)\n",
        "    \n",
        "    # Make a plot for each sample.\n",
        "    for i in range(4):\n",
        "        \n",
        "        # PCA-created features.\n",
        "        x_sub = ypred_sc['pca_f1_sample{}'.format(i)]\n",
        "        y_sub = ypred_sc['pca_f2_sample{}'.format(i)]\n",
        "        \n",
        "        # Cluster assignments.\n",
        "        c = ypred_sc['clust{}_sample{}'.format(cluster, i)]\n",
        "        \n",
        "        # Assign the subplot to its place on the grid.\n",
        "        rows = int(np.floor(i / 2))\n",
        "        cols = i % 2\n",
        "        axarr[rows, cols].scatter(x_sub, y_sub, c=c)\n",
        "        axarr[rows, cols].set_title('sample {}'.format(i))\n",
        "        axarr[rows, cols].set_xlim([-.5, .5])\n",
        "        axarr[rows, cols].set_ylim([-.5, 1.2])\n",
        "    \n",
        "    # Space out the plots so that the headings don't overlap axis values.\n",
        "    plt.suptitle('{} Clusters'.format(cluster), fontsize=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vd0zG7dqZFso",
        "colab_type": "text"
      },
      "source": [
        "Watching the latter plots of the 2-feature PCA data, the 2 and 3 cluster configuration show a consistent solution across all samples. The 4 and 5 cluster configuration show overfitting for all samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0W1VNM8NZHTf",
        "colab_type": "text"
      },
      "source": [
        "**Affinity Propagation**\n",
        "\n",
        "For the Affinity Propagation model, we allow the model to find the k number of cluster, and later we calculate the corresponding Silhouette scores for each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtnPPgGkZMpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize data frames\n",
        "ypred = pd.DataFrame()\n",
        "score_af = pd.DataFrame(columns=['cluster_pred','AF'])\n",
        "\n",
        "# Keep track of counts of the models and use data from the different folds\n",
        "for counter, data in enumerate([X1, X2, X3, X4]):\n",
        "    # Ensuring all sets are the same lenght\n",
        "    data = data[:4013][:]\n",
        "    # Instantiating and fit_predicting model to then add to data frame\n",
        "    af = AffinityPropagation().fit(data)\n",
        "    cluster_centers_indices = af.cluster_centers_indices_\n",
        "    n_clusters_ = len(cluster_centers_indices)\n",
        "    #pred = af.fit_predict(data)\n",
        "    #ypred['clust' + str(nclust) + '_sample' + str(counter)] = pred\n",
        "    # Calculating silhouette scores for the data and adding that to the shilouette score\n",
        "    labels = af.labels_\n",
        "    sscore_af = metrics.silhouette_score(data, labels, metric='euclidean')\n",
        "    score_af = score_af.append({'cluster_pred':'clust' + str(n_clusters_) + '_sample' + str(counter), \n",
        "                              'AF':sscore_af}, ignore_index=True)\n",
        "\n",
        "score_af.sort_values(by='AF', ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xP4HI6wZT37",
        "colab_type": "text"
      },
      "source": [
        "The number of clusters generated seem absurd, suggesting it isn't reliable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWMM50VYZj1b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Calculate predicted values.\n",
        "y_pred = KMeans(n_clusters=4, random_state=42).fit_predict(X_norm)\n",
        "\n",
        "df_y = pd.DataFrame(y_pred)\n",
        "df_y.columns = ['Cluster']\n",
        "\n",
        "# Add the outcome back onto X\n",
        "combined = X.join(df_y, how='inner')\n",
        "combined.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fqTkXWMGZmQ_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create age buckets\n",
        "\n",
        "def age_bucket(age):\n",
        "    output = ''\n",
        "    if age <=20:\n",
        "        output = 'Under 20'\n",
        "    elif (age > 20 and age <= 30):\n",
        "        output = 'Between 20 and 30'\n",
        "    elif (age > 30 and age <= 40):\n",
        "        output = 'Between 30 and 40'\n",
        "    elif (age > 40 and age <= 50):\n",
        "        output = 'Between 40 and 50'\n",
        "    elif (age > 50 and age <= 60):\n",
        "        output = 'Between 50 and 60'\n",
        "    else:\n",
        "        output = 'Over 60'\n",
        "    \n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQhSYOh5aIp2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combined['Age Bucket'] = combined['age'].apply(lambda x: age_bucket(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26kmpabvZ80P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Look at Gender Breakdown for Count\n",
        "g = sns.factorplot(x='Age Bucket', col='Cluster', kind=\"count\", data=combined, size=4)\n",
        "g.set_xticklabels(rotation=90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQU64qwaav-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.factorplot(x='gender', col='Cluster', kind=\"count\", data=combined, size=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jhBDFWbbDQe",
        "colab_type": "text"
      },
      "source": [
        "Considering 0 = Female, 1 = Male, we can observe that in cluster 3 (best records), there is a predominant number of male runners. We confirm with following count values:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkGdbKq0a3uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Separate the clusters\n",
        "cluster0 = combined[combined['Cluster']==0]\n",
        "cluster1 = combined[combined['Cluster']==1]\n",
        "cluster2 = combined[combined['Cluster']==2]\n",
        "cluster3 = combined[combined['Cluster']==3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "honWkQRKbHkF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check Gender Breakdown\n",
        "cluster0['gender'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1eDy5AjvbM85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster1['gender'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS79EXtSbSx1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster2['gender'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBOVZqBcbYNl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cluster3['gender'].value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ev6sCHsPcLKI",
        "colab_type": "text"
      },
      "source": [
        "**K-prototype**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO5WLfvb53vI",
        "colab_type": "text"
      },
      "source": [
        "**Hopkins Statistics**\n",
        "\n",
        "To understand if the dataset can be clustered, we used the Hopkins statistic, which tests the spatial randomness of the data and indicates the cluster tendency or how well the data can be clustered. It calculates the probability that a given data is generated by a uniform distribution (Alboukadel Kassambara, n.d.). The inference is as follows for a data of dimensions d:\n",
        "\n",
        "- If the value is around 0.5 or lesser, the data is uniformly distributed and hence it is unlikely to have statistically significant clusters.\n",
        "\n",
        "- If the value is between {0.7, ..., 0.99}, it has a high tendency to cluster and therefore likely to have statistically significant clusters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xaPYIBz52zh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Hopkins Statistic is a way of measuring the cluster tendency of a data set.\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from random import sample\n",
        "from numpy.random import uniform\n",
        "import numpy as np\n",
        "from math import isnan\n",
        " \n",
        "def hopkins(X):\n",
        "    d = X.shape[1]\n",
        "    #d = len(vars) # columns\n",
        "    n = len(X) # rows\n",
        "    m = int(0.1 * n) # heuristic from article [1]\n",
        "    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n",
        " \n",
        "    rand_X = sample(range(0, n, 1), m)\n",
        " \n",
        "    ujd = []\n",
        "    wjd = []\n",
        "    for j in range(0, m):\n",
        "        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n",
        "        ujd.append(u_dist[0][1])\n",
        "        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n",
        "        wjd.append(w_dist[0][1])\n",
        " \n",
        "    H = sum(ujd) / (sum(ujd) + sum(wjd))\n",
        "    if isnan(H):\n",
        "        print(ujd, wjd)\n",
        "        H = 0\n",
        " \n",
        "    return H"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzBiPCCp6JgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Use a random sample of Data for faster computation\n",
        "data = data.sample(20000,random_state=41)\n",
        "data.head()\n",
        "#Resetting the indexs\n",
        "data=data.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQCvMn3A6Mln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Num_features =data.select_dtypes(include=[np.number]).columns\n",
        "hopkins(data[Num_features])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8ppXZ5v6Qll",
        "colab_type": "text"
      },
      "source": [
        "Result: This test is run on all the numerical variables of the entire dataset and the test statistic we got is 0.99 which indicates that data has a high tendency to cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5Ip-cdNI8DB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.gender = data.gender.map(lambda x: 0 if x is 'F' else 1)\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBLJHpBD6PNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Selection of variables for PCA\n",
        "Data_pca= data[['age', 'pace', 'division', 'overall', 'gender']]\n",
        "print (Data_pca.dtypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHFbRHFR6Xcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Principal Component\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=4, whiten=True)\n",
        "Num_features=Data_pca.select_dtypes(include=[np.number]).columns\n",
        "x=Data_pca[Num_features]\n",
        "principalComponents = pca.fit_transform(x)\n",
        "\n",
        "# Cumulative Explained Variance\n",
        "cum_explained_var = []\n",
        "for i in range(0, len(pca.explained_variance_ratio_)):\n",
        "    if i == 0:\n",
        "        cum_explained_var.append(pca.explained_variance_ratio_[i])\n",
        "    else:\n",
        "        cum_explained_var.append(pca.explained_variance_ratio_[i] + \n",
        "                                 cum_explained_var[i-1])\n",
        "\n",
        "print(cum_explained_var)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asPZ4GDF6amY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Principal Components converted to a Data frame\n",
        "principalDf  = pd.DataFrame(data = principalComponents\n",
        "             , columns = ['principal component 1', 'principal component 2', 'principal component 3', 'principal component 4'])\n",
        "principalDf.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCpuQLmW7C3I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Concatenating the PCAs with the categorical variable\n",
        "finalDf_Cat = pd.concat([principalDf, Data_pca['gender']], axis = 1)\n",
        "finalDf_Cat.head(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7CGp0oo6gQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install kmodes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F31VobmV6jkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Choosing optimal K value\n",
        "from kmodes.kprototypes import KPrototypes\n",
        "\n",
        "cost = []\n",
        "X = finalDf_Cat\n",
        "for num_clusters in list(range(2,7)):\n",
        "    kproto = KPrototypes(n_clusters=num_clusters, init='Huang', random_state=42,n_jobs=-2,max_iter=15,n_init=50) \n",
        "    kproto.fit_predict(X, categorical=[3])\n",
        "    cost.append(kproto.cost_)\n",
        "\n",
        "plt.plot(cost)\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('cost')\n",
        "plt.show"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV27KxIT6nEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the dataset into matrix\n",
        "X = finalDf_Cat.as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvVrEUOC7x_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Running K-Prototype clustering\n",
        "kproto = KPrototypes(n_clusters=2, init='Huang', verbose=0, random_state=42,max_iter=20, n_init=50,n_jobs=-2,gamma=.25) \n",
        "clusters = kproto.fit_predict(X, categorical=[3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY9AHDZq70NT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualize K-Prototype clustering on the PCA projected Data\n",
        "df=pd.DataFrame(finalDf_Cat)\n",
        "df['Cluster_id']=clusters\n",
        "print(df['Cluster_id'].value_counts())\n",
        "sns.pairplot(df,hue='Cluster_id',palette='Dark2',diag_kind='kde')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}